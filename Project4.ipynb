{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Decision Trees and Random Forest\n",
    "\n",
    "### Name:\n",
    "### Course Level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore the application of classification using: a) Decision Tree and b) Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to develop software modules for classification of Titanic passengers via decision trees and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first problem we aim to us Information Gain to build a decision tree for classification.  The tree will complete when one of two conditions are met:\n",
    "\n",
    "1. The maximum tree depth is reached.\n",
    "2. The minimum number of samples is reached in a given leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (70pts)**\n",
    "\n",
    "1 (5pts). The first thing you'll need to do is read the Titatinic dataset from D2L (details about the dataset can be found [Here](https://www.kaggle.com/c/titanic/data?select=test.csv)).  Note that you will need both the train.csv (to build your tree) and test.csv to evaluate the classificaiton accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data #\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (20pts). Next, we need a couple helper functions to compute the conditional entropy and information gain.\n",
    "\n",
    "**Recall: Entropy** \n",
    "$$\n",
    "    H(X) = -\\sum_{\\textbf{x} \\in X} p(\\textbf{x}) \\log_2( p(\\textbf{x}) )\n",
    "$$\n",
    "\n",
    "**Recall: Conditional Entropy** \n",
    "$$\n",
    "    H(Y|X=\\textbf{x}) = -\\sum_{\\textbf{y} \\in Y} p(\\textbf{y}|\\textbf{x}) \\log_2 ( p(\\textbf{y}|\\textbf{x}) )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    p(\\textbf{y}|\\textbf{x}) = \\frac{p(\\textbf{y},\\textbf{x})}{p(\\textbf{x})}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    p(\\textbf{x}) = \\sum_{\\textbf{y} \\in Y} p(\\textbf{y},\\textbf{x})\n",
    "$$\n",
    "\n",
    "**Recall: Information Gain** \n",
    "$$\n",
    "    IG(Y|X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions here (you may want to modify the input parameters depending on your implimentation #\n",
    "def Entropy(d_X,d_Y):\n",
    "    counts = Counter(d_Y) # Number of times each label appears\n",
    "    total = len(d_Y)      # Number of samples\n",
    "    H = -sum((count/total) * np.log2(count/total) for count in counts.values() if count > 0)\n",
    "    return H\n",
    "\n",
    "def C_Entropy(d_X,d_Y):\n",
    "    total = len(d_Y)\n",
    "    unique_vals = set(d_X) # Unique vals for the feature we're splitting on\n",
    "    cH = 0.0\n",
    "    for val in unique_vals:\n",
    "        # For each unique value in the feature, find the corresponding labels in d_Y\n",
    "        subset_y = [d_Y[i] for i in range(total) if d_X[i] == val]\n",
    "        weight = len(subset_y) / total\n",
    "        cH += weight * Entropy(d_X, subset_y) # Add weighted entropy of subset\n",
    "    return cH\n",
    "\n",
    "def IG(d_X,d_Y):\n",
    "    IG = Entropy(d_X, d_Y) - C_Entropy(d_X, d_Y)\n",
    "    return IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (30pts). Define a function to build the tree, recall there should be one of two exit criterion:\n",
    "\n",
    "1. The maximum tree depth is reached\n",
    "2. The minimum number of samples is reached in a given leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of tree node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, prediction=None, is_numeric=False):\n",
    "        self.feature = feature              # Feature that node splits on\n",
    "        self.threshold = threshold          # Threshold value (for numerical splits)\n",
    "        self.left = left                    # Left child (<= threshold or == value)\n",
    "        self.right = right                  # Right child (> threshold or != value)\n",
    "        self.prediction = prediction        # Label prediction (for leaf nodes)\n",
    "        self.is_numeric = is_numeric        # True if the split feature is numeric\n",
    "\n",
    "# Returns the most common class label in a given list.\n",
    "# Used when creating a leaf node to make a prediction.\n",
    "def majority(l_labels):\n",
    "    return Counter(l_labels).most_common(1)[0][0]\n",
    "\n",
    "# Finds the best feature and split point that maximizes information gain.\n",
    "def find_best_split(d_data, l_labels):\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_gain = -1\n",
    "    is_numeric_split = False\n",
    "\n",
    "    for feature in d_data.columns:\n",
    "        X_col = d_data[feature].tolist()\n",
    "\n",
    "        if isinstance(X_col[0], (int, float)):  # If feature is numeric\n",
    "            unique_values = sorted(set(X_col))\n",
    "            # Compute mdpts between all adj unique values\n",
    "            thresholds = [(unique_values[i] + unique_values[i+1])/2 for i in range(len(unique_values)-1)]\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Split labels based on threshold\n",
    "                left_y = [l_labels[i] for i in range(len(X_col)) if X_col[i] <= threshold]\n",
    "                right_y = [l_labels[i] for i in range(len(X_col)) if X_col[i] > threshold]\n",
    "\n",
    "                # Screw invalid splits\n",
    "                if not left_y or not right_y:\n",
    "                    continue\n",
    "\n",
    "                total = len(l_labels)\n",
    "                # Weighted conditional entropy for split\n",
    "                cH = (len(left_y)/total) * Entropy(None, left_y) + (len(right_y)/total) * Entropy(None, right_y)\n",
    "                gain = Entropy(None, l_labels) - cH\n",
    "\n",
    "                # Update best split if current is better\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    is_numeric_split = True\n",
    "        else:  # If feature is categorical\n",
    "            gain = IG(X_col, l_labels)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = X_col[0]  # Val to test equality against\n",
    "                is_numeric_split = False\n",
    "\n",
    "    return best_feature, best_threshold, best_gain, is_numeric_split\n",
    "\n",
    "# Recursively build the decision tree\n",
    "def BuildTree(d_data, l_labels, depth=0, max_depth=5, min_samples=10):\n",
    "    # Stopping criteria: all labels are the same, or we've hit a depth/sample limit\n",
    "    if len(set(l_labels)) == 1 or depth >= max_depth or len(d_data) <= min_samples:\n",
    "        return Node(prediction=majority(l_labels))  # Make a leaf node with majority label\n",
    "\n",
    "    # Find best split\n",
    "    feature, threshold, gain, is_numeric = find_best_split(d_data, l_labels)\n",
    "\n",
    "    # If no split improves gain, make a leaf node\n",
    "    if gain == 0 or feature is None:\n",
    "        return Node(prediction=majority(l_labels))\n",
    "\n",
    "    # Split data on best feature and threshold\n",
    "    if is_numeric:\n",
    "        left_indices = d_data[feature] <= threshold\n",
    "        right_indices = d_data[feature] > threshold\n",
    "    else:\n",
    "        left_indices = d_data[feature] == threshold\n",
    "        right_indices = d_data[feature] != threshold\n",
    "\n",
    "    # Slice dataset and labels accordingly\n",
    "    left_data = d_data[left_indices]\n",
    "    right_data = d_data[right_indices]\n",
    "    left_labels = [l_labels[i] for i in range(len(l_labels)) if left_indices.iloc[i]]\n",
    "    right_labels = [l_labels[i] for i in range(len(l_labels)) if right_indices.iloc[i]]\n",
    "\n",
    "    # Recursively build left and right subtrees\n",
    "    left_subtree = BuildTree(left_data, left_labels, depth + 1, max_depth, min_samples)\n",
    "    right_subtree = BuildTree(right_data, right_labels, depth + 1, max_depth, min_samples)\n",
    "\n",
    "    # Return a node with references to its subtrees\n",
    "    return Node(feature=feature, threshold=threshold, left=left_subtree, right=right_subtree, is_numeric=is_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (15pts). Next write a function to perform the prediciton (classificaiton) of a new training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class label of a single sample using a trained decision tree.\n",
    "def DT_predict(d_tree, d_sample):\n",
    "    # Traverse until we reach a leaf node with a prediction\n",
    "    while d_tree.prediction is None:\n",
    "        # If current node splits on numeric feature\n",
    "        if d_tree.is_numeric:\n",
    "            # Go left if sample value is less <= threshold\n",
    "            if d_sample[d_tree.feature] <= d_tree.threshold:\n",
    "                d_tree = d_tree.left\n",
    "            else:\n",
    "                d_tree = d_tree.right\n",
    "        else:\n",
    "            # For categorical features, test for equality\n",
    "            if d_sample[d_tree.feature] == d_tree.threshold:\n",
    "                d_tree = d_tree.left\n",
    "            else:\n",
    "                d_tree = d_tree.right\n",
    "\n",
    "    # When we reach a leaf node, return its prediction\n",
    "    return d_tree.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 (5pts). Using the test set, evaluate the accuracy of your classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.03349282296651 %\n"
     ]
    }
   ],
   "source": [
    "# Accuracy = (# of correct predictions) / (total samples)\n",
    "def accuracy(t_test, t_tree):\n",
    "    correct = 0  # Num of correct predictions\n",
    "    \n",
    "    for _, row in t_test.iterrows():\n",
    "        true_label = row[\"Survived\"]       # Actual label\n",
    "        sample = row.drop(\"Survived\")      # Feature values only\n",
    "        pred = DT_predict(t_tree, sample)  # Predict using decision tree\n",
    "        correct += (pred == true_label)    # Count+ if prediction is correct\n",
    "\n",
    "    a_accuracy = correct / len(t_test)\n",
    "    return a_accuracy\n",
    "\n",
    "# Preprocesses the Titanic dataset & remove irrelevant features and handle missing data + encoding.\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop columns not used in our model\n",
    "    df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], inplace=True)\n",
    "\n",
    "    # Fill missing values in 'Age' with median age\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "\n",
    "    # Fill missing values in 'Embarked' with the most frequent embarkation point\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
    "\n",
    "    # Convert categorical features into numeric\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].map({\"C\": 0, \"Q\": 1, \"S\": 2})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing to train and test data\n",
    "train_data = preprocess(train_df)\n",
    "test_data = preprocess(test_df)\n",
    "\n",
    "# Separate features and labels from training data\n",
    "X_train = train_data.drop(columns=[\"Survived\"])\n",
    "y_train = train_data[\"Survived\"].tolist()\n",
    "\n",
    "# Build decision tree using train data\n",
    "DTree = BuildTree(X_train, y_train)\n",
    "\n",
    "# If 'Survived' is missing in test data, add dummy vals to enable evaluation\n",
    "if \"Survived\" not in test_data.columns:\n",
    "    test_data[\"Survived\"] = [0] * len(test_data)  # dummy labels\n",
    "\n",
    "# Evaluate model on the test data\n",
    "a_accuracy = accuracy(test_data, DTree)\n",
    "print(f\"Accuracy: {a_accuracy * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem B (30pts)**\n",
    "* In Problem A, you designed a function to build a decision tree.  In this problem, you will use that function to build several different trees via bagging (bootstrapping and aggregation).  The result will be a random forest.  You should **at minimum** have a forest of at least 5 trees. \n",
    "\n",
    "* Once the forest is built, similar to A.5, design a function for comparing the accuracy of a standard tree, vs. random forest on the testing dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait! This may take a while :)\n",
      "Decision Tree Accuracy: 77.03349282296651 %\n",
      "Random Forest Accuracy: 77.27272727272727 %\n"
     ]
    }
   ],
   "source": [
    "# Generate a bootstrap sample (random sampling with replacement)\n",
    "# Helps simulate the idea of training each tree on a slightly different dataset\n",
    "def bootstrap_sample(df):\n",
    "    indices = [random.randint(0, len(df) - 1) for _ in range(len(df))]\n",
    "    return df.iloc[indices]\n",
    "\n",
    "# Build Random Forest using bootstrapped datasets and random feature subsets\n",
    "def BuildRandomForest(data, labels, num_trees, max_depth):\n",
    "    min_samples = 5\n",
    "    feature_subset_ratio = 0.6\n",
    "    \n",
    "    forest = []\n",
    "\n",
    "    for _ in range(num_trees):\n",
    "        # Bootstrap data: random rows sampled with replacement\n",
    "        indices = np.random.choice(len(data), size=len(data), replace=True)\n",
    "        sample_data = data.iloc[indices]\n",
    "        sample_labels = [labels[i] for i in indices]\n",
    "\n",
    "        # Random subspace: randomly select a subset of features for this tree\n",
    "        features = data.columns.tolist()\n",
    "        subset_size = max(1, int(len(features) * feature_subset_ratio))\n",
    "        selected_features = np.random.choice(features, size=subset_size, replace=False)\n",
    "\n",
    "        # Build decision tree using sampled data and selected features\n",
    "        tree = BuildTree(sample_data[selected_features], sample_labels,\n",
    "                         max_depth=max_depth, min_samples=min_samples)\n",
    "\n",
    "        # Store tree along with features trained on\n",
    "        forest.append((tree, selected_features))\n",
    "\n",
    "    return forest\n",
    "\n",
    "# Predict label of a sample using trees in forest\n",
    "def RF_predict(forest, sample):\n",
    "    predictions = []\n",
    "\n",
    "    for tree, features in forest:\n",
    "        # Extract only the subset of features this tree was trained on\n",
    "        tree_sample = sample[features]\n",
    "        predictions.append(DT_predict(tree, tree_sample))\n",
    "\n",
    "    # Return the most common prediction (majority vote)\n",
    "    return Counter(predictions).most_common(1)[0][0]\n",
    "\n",
    "# Evaluate accuracy of random forest on test dataset\n",
    "def RF_accuracy(test_data, forest):\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "\n",
    "    for idx, row in test_data.iterrows():\n",
    "        true_label = row[\"Survived\"]        # Ground truth\n",
    "        sample = row.drop(\"Survived\")       # Feature values only\n",
    "        pred = RF_predict(forest, sample)   # Predict using the forest\n",
    "\n",
    "        if pred == true_label:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Building and Evaluating\n",
    "print(\"Wait! This may take a while :)\")\n",
    "\n",
    "# Train random forest of 5 trees using training data\n",
    "forest = BuildRandomForest(X_train, y_train, num_trees=50, max_depth=10)\n",
    "\n",
    "# Call accuracy from A.5 using single decision tree\n",
    "dt_acc = accuracy(test_data, DTree)\n",
    "\n",
    "# Call accuracy from B using random forest\n",
    "rf_acc = RF_accuracy(test_data, forest)\n",
    "\n",
    "# Print both results for comparison\n",
    "print(f\"Decision Tree Accuracy: {dt_acc * 100} %\")\n",
    "print(f\"Random Forest Accuracy: {rf_acc * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
